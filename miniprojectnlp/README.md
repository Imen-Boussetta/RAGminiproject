# üìÑ RAG PDF Q/A - Mini-Projet NLP

Application web de Question/R√©ponse sur documents PDF utilisant le syst√®me RAG (Retrieval Augmented Generation) avec Ollama et JavaScript.

##  Description

Ce projet impl√©mente un chatbot intelligent capable de r√©pondre √† des questions sur le contenu d'un document PDF. Il utilise la technique RAG qui combine :
- **Retrieval** : Recherche s√©mantique dans le document via des embeddings vectoriels
- **Augmented** : Enrichissement du contexte avec les passages pertinents
- **Generation** : G√©n√©ration de r√©ponses par un mod√®le de langage (LLM)

##  Fonctionnalit√©s

-  **Upload de PDF** : Importez vos documents PDF
-  **Indexation automatique** : Le texte est extrait, d√©coup√© en chunks et vectoris√©
-  **Questions/R√©ponses** : Posez des questions en langage naturel
-  **Recherche s√©mantique** : Trouve les passages pertinents via similarit√© cosinus
-  **G√©n√©ration intelligente** : R√©ponses contextualis√©es par Llama 3.2
-  **Interface minimaliste** : Design √©pur√© et responsive

##  Technologies

### Backend
- **Node.js** v24+ (support natif de `fetch`)
- **Express** 5.2.1 - Framework web
- **Multer** 2.0.2 - Upload de fichiers
- **pdf-parse** 2.4.5 - Extraction de texte PDF

### IA & NLP
- **Ollama** - Serveur LLM local
  - `llama3.2` - G√©n√©ration de r√©ponses
  - `nomic-embed-text` - Embeddings vectoriels (768 dimensions)

### Frontend
- HTML5, CSS3, JavaScript vanilla
- Fetch API pour les requ√™tes AJAX

##  Pr√©requis

1. **Node.js** : Version 24.13.0 ou sup√©rieure
   ```bash
   node --version
   ```

2. **Ollama** : [Installer Ollama](https://ollama.ai)
   ```bash
   # T√©l√©charger les mod√®les n√©cessaires
   ollama pull llama3.2
   ollama pull nomic-embed-text
   
   # V√©rifier l'installation
   ollama list
   ```

##  Installation

1. **Cloner ou t√©l√©charger le projet**
   ```bash
   cd miniprojectnlp
   ```

2. **Installer les d√©pendances**
   ```bash
   npm install
   ```

3. **D√©marrer Ollama** (si ce n'est pas d√©j√† fait)
   ```bash
   ollama serve
   ```

4. **Lancer le serveur**
   ```bash
   npm start
   ```

5. **Ouvrir dans le navigateur**
   ```
   http://localhost:3000
   ```

##  Utilisation

### 1. Indexer un PDF

1. Cliquez sur **"Choisir un fichier"**
2. S√©lectionnez votre PDF
3. Cliquez sur **"Indexer"**
4. Attendez le message de confirmation : `‚úì X chunks index√©s`

### 2. Poser des questions

1. Tapez votre question dans le champ de texte
2. Cliquez sur **"Demander"**
3. La r√©ponse s'affiche avec les sources utilis√©es

### Exemple

**Question** : `Quels sont les objectifs de ce projet ?`

**R√©ponse** : Le syst√®me analyse le PDF, trouve les passages pertinents et g√©n√®re une r√©ponse contextualis√©e.

##  Structure du projet

```
miniprojectnlp/
‚îú‚îÄ‚îÄ server.js           # Serveur Express (API REST)
‚îú‚îÄ‚îÄ rag.js             # Logique RAG (embeddings, chunking, Q/A)
‚îú‚îÄ‚îÄ package.json       # D√©pendances et scripts
‚îú‚îÄ‚îÄ README.md          # Documentation du projet
‚îú‚îÄ‚îÄ data/              # Donn√©es g√©n√©r√©es
‚îÇ   ‚îú‚îÄ‚îÄ index.json     # Index vectoriel (chunks + embeddings)
‚îÇ   ‚îî‚îÄ‚îÄ uploads/       # PDFs upload√©s
‚îî‚îÄ‚îÄ public/            # Frontend
    ‚îú‚îÄ‚îÄ index.html     # Interface utilisateur
    ‚îú‚îÄ‚îÄ style.css      # Styles minimalistes
    ‚îî‚îÄ‚îÄ app.js         # Logique frontend (fetch API)
```

##  Architecture RAG

### Pipeline d'indexation

```
PDF ‚Üí Extraction texte ‚Üí Chunking ‚Üí Embeddings ‚Üí Sauvegarde JSON
```

1. **Extraction** : `pdf-parse` lit le PDF et extrait le texte
2. **Chunking** : D√©coupage en morceaux de 1200 caract√®res (overlap de 200)
3. **Embeddings** : Vectorisation via `nomic-embed-text` (768 dimensions)
4. **Stockage** : Sauvegarde dans `data/index.json`

### Pipeline Question/R√©ponse

```
Question ‚Üí Embedding ‚Üí Recherche similarit√© ‚Üí Top-K chunks ‚Üí LLM ‚Üí R√©ponse
```

1. **Vectorisation** : La question est transform√©e en embedding
2. **Retrieval** : Calcul de similarit√© cosinus avec tous les chunks
3. **Top-K** : S√©lection des 5 chunks les plus pertinents
4. **G√©n√©ration** : `llama3.2` g√©n√®re la r√©ponse √† partir du contexte

##  Calcul de similarit√©

La similarit√© cosinus mesure l'angle entre deux vecteurs :

```javascript
cosŒ∏ = (A ¬∑ B) / (||A|| √ó ||B||)
```

- **1.0** : Vecteurs identiques (tr√®s pertinent)
- **0.0** : Vecteurs orthogonaux (non pertinent)
- **-1.0** : Vecteurs oppos√©s

##  API Routes

### `GET /api/health`
V√©rification de l'√©tat du serveur

**R√©ponse** :
```json
{
  "ok": true,
  "message": "Serveur RAG op√©rationnel"
}
```

### `POST /api/index`
Indexation d'un PDF

**Body** : `FormData` avec fichier PDF (cl√©: `pdf`)

**R√©ponse** :
```json
{
  "ok": true,
  "indexPath": "data/index.json",
  "stats": {
    "chunks": 42,
    "embedModel": "nomic-embed-text",
    "source": "document.pdf"
  }
}
```

### `POST /api/ask`
Question sur le PDF index√©

**Body** :
```json
{
  "question": "Quels sont les objectifs ?"
}
```

**R√©ponse** :
```json
{
  "answer": "Les objectifs sont...",
  "sources": [
    {
      "id": "document.pdf::chunk_5",
      "score": 0.89,
      "text": "..."
    }
  ]
}
```

##  Configuration

### Param√®tres RAG (dans `rag.js`)

```javascript
// Chunking
chunkSize: 1200        // Taille d'un chunk (caract√®res)
chunkOverlap: 200      // Chevauchement entre chunks

// Retrieval
topK: 5                // Nombre de chunks √† r√©cup√©rer

// Mod√®les
embedModel: 'nomic-embed-text'  // Mod√®le d'embeddings
chatModel: 'llama3.2'            // Mod√®le de g√©n√©ration
```

### Port du serveur

Par d√©faut : `3000`

Pour changer :
```bash
PORT=8080 npm start
```

##  D√©pannage

### Ollama n'est pas accessible
```bash
# V√©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# Red√©marrer Ollama si n√©cessaire
ollama serve
```

### Erreur "pdfParse is not a function"
- Assurez-vous d'avoir la bonne version de `pdf-parse`
- Red√©marrez le serveur apr√®s modification

### Le PDF n'est pas index√©
- V√©rifiez que le fichier est bien un PDF
- Consultez les logs du serveur pour les erreurs d√©taill√©es

### R√©ponses de mauvaise qualit√©
- Augmentez `topK` pour plus de contexte
- Ajustez `chunkSize` et `chunkOverlap`
- Essayez un mod√®le LLM plus performant

##  Concepts NLP

### Embeddings
Repr√©sentation vectorielle du texte qui capture le sens s√©mantique. Des textes similaires auront des vecteurs proches.

### Chunking
D√©coupage du texte en morceaux g√©rables par le LLM. L'overlap √©vite de perdre le contexte aux fronti√®res.

### RAG vs Fine-tuning
- **RAG** : Pas besoin de r√©entra√Æner, ajout dynamique de connaissances
- **Fine-tuning** : R√©entra√Ænement co√ªteux, connaissances fig√©es

### Vector Store
Base de donn√©es contenant les chunks avec leurs embeddings pour une recherche rapide.

##  Ressources

- [Ollama Documentation](https://ollama.ai/docs)
- [RAG Explained](https://arxiv.org/abs/2005.11401)
- [Langchain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

##  Auteur

**Imen Boussetta**

Projet d√©velopp√© dans le cadre d'une formation en Intelligence Artificielle (LLM, LangChain, RAG, Agents).

##  Licence

Ce projet est √† usage √©ducatif.


