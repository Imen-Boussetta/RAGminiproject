# ğŸ“„ RAG PDF Q/A - Mini-Projet NLP

Application web de Question/RÃ©ponse sur documents PDF utilisant le systÃ¨me RAG (Retrieval Augmented Generation) avec Ollama et JavaScript.

## ğŸ¯ Description

Ce projet implÃ©mente un chatbot intelligent capable de rÃ©pondre Ã  des questions sur le contenu d'un document PDF. Il utilise la technique RAG qui combine :
- **Retrieval** : Recherche sÃ©mantique dans le document via des embeddings vectoriels
- **Augmented** : Enrichissement du contexte avec les passages pertinents
- **Generation** : GÃ©nÃ©ration de rÃ©ponses par un modÃ¨le de langage (LLM)

## âœ¨ FonctionnalitÃ©s

- ğŸ“¤ **Upload de PDF** : Importez vos documents PDF
- ğŸ” **Indexation automatique** : Le texte est extrait, dÃ©coupÃ© en chunks et vectorisÃ©
- ğŸ’¬ **Questions/RÃ©ponses** : Posez des questions en langage naturel
- ğŸ¯ **Recherche sÃ©mantique** : Trouve les passages pertinents via similaritÃ© cosinus
- ğŸ¤– **GÃ©nÃ©ration intelligente** : RÃ©ponses contextualisÃ©es par Llama 3.2
- ğŸ¨ **Interface minimaliste** : Design Ã©purÃ© et responsive

## ğŸ› ï¸ Technologies

### Backend
- **Node.js** v24+ (support natif de `fetch`)
- **Express** 5.2.1 - Framework web
- **Multer** 2.0.2 - Upload de fichiers
- **pdf-parse** 2.4.5 - Extraction de texte PDF

### IA & NLP
- **Ollama** - Serveur LLM local
  - `llama3.2` - GÃ©nÃ©ration de rÃ©ponses
  - `nomic-embed-text` - Embeddings vectoriels (768 dimensions)

### Frontend
- HTML5, CSS3, JavaScript vanilla
- Fetch API pour les requÃªtes AJAX

## ğŸ“‹ PrÃ©requis

1. **Node.js** : Version 24.13.0 ou supÃ©rieure
   ```bash
   node --version
   ```

2. **Ollama** : [Installer Ollama](https://ollama.ai)
   ```bash
   # TÃ©lÃ©charger les modÃ¨les nÃ©cessaires
   ollama pull llama3.2
   ollama pull nomic-embed-text
   
   # VÃ©rifier l'installation
   ollama list
   ```

## ğŸš€ Installation

1. **Cloner ou tÃ©lÃ©charger le projet**
   ```bash
   cd miniprojectnlp
   ```

2. **Installer les dÃ©pendances**
   ```bash
   npm install
   ```

3. **DÃ©marrer Ollama** (si ce n'est pas dÃ©jÃ  fait)
   ```bash
   ollama serve
   ```

4. **Lancer le serveur**
   ```bash
   npm start
   ```

5. **Ouvrir dans le navigateur**
   ```
   http://localhost:3000
   ```

## ğŸ“– Utilisation

### 1. Indexer un PDF

1. Cliquez sur **"Choisir un fichier"**
2. SÃ©lectionnez votre PDF
3. Cliquez sur **"Indexer"**
4. Attendez le message de confirmation : `âœ“ X chunks indexÃ©s`

### 2. Poser des questions

1. Tapez votre question dans le champ de texte
2. Cliquez sur **"Demander"**
3. La rÃ©ponse s'affiche avec les sources utilisÃ©es

### Exemple

**Question** : `Quels sont les objectifs de ce projet ?`

**RÃ©ponse** : Le systÃ¨me analyse le PDF, trouve les passages pertinents et gÃ©nÃ¨re une rÃ©ponse contextualisÃ©e.

## ğŸ“ Structure du projet

```
miniprojectnlp/
â”œâ”€â”€ server.js           # Serveur Express (API REST)
â”œâ”€â”€ rag.js             # Logique RAG (embeddings, chunking, Q/A)
â”œâ”€â”€ package.json       # DÃ©pendances et scripts
â”œâ”€â”€ README.md          # Documentation du projet
â”œâ”€â”€ data/              # DonnÃ©es gÃ©nÃ©rÃ©es
â”‚   â”œâ”€â”€ index.json     # Index vectoriel (chunks + embeddings)
â”‚   â””â”€â”€ uploads/       # PDFs uploadÃ©s
â””â”€â”€ public/            # Frontend
    â”œâ”€â”€ index.html     # Interface utilisateur
    â”œâ”€â”€ style.css      # Styles minimalistes
    â””â”€â”€ app.js         # Logique frontend (fetch API)
```

## ğŸ”§ Architecture RAG

### Pipeline d'indexation

```
PDF â†’ Extraction texte â†’ Chunking â†’ Embeddings â†’ Sauvegarde JSON
```

1. **Extraction** : `pdf-parse` lit le PDF et extrait le texte
2. **Chunking** : DÃ©coupage en morceaux de 1200 caractÃ¨res (overlap de 200)
3. **Embeddings** : Vectorisation via `nomic-embed-text` (768 dimensions)
4. **Stockage** : Sauvegarde dans `data/index.json`

### Pipeline Question/RÃ©ponse

```
Question â†’ Embedding â†’ Recherche similaritÃ© â†’ Top-K chunks â†’ LLM â†’ RÃ©ponse
```

1. **Vectorisation** : La question est transformÃ©e en embedding
2. **Retrieval** : Calcul de similaritÃ© cosinus avec tous les chunks
3. **Top-K** : SÃ©lection des 5 chunks les plus pertinents
4. **GÃ©nÃ©ration** : `llama3.2` gÃ©nÃ¨re la rÃ©ponse Ã  partir du contexte

## ğŸ§® Calcul de similaritÃ©

La similaritÃ© cosinus mesure l'angle entre deux vecteurs :

```javascript
cosÎ¸ = (A Â· B) / (||A|| Ã— ||B||)
```

- **1.0** : Vecteurs identiques (trÃ¨s pertinent)
- **0.0** : Vecteurs orthogonaux (non pertinent)
- **-1.0** : Vecteurs opposÃ©s

## ğŸ“Š API Routes

### `GET /api/health`
VÃ©rification de l'Ã©tat du serveur

**RÃ©ponse** :
```json
{
  "ok": true,
  "message": "Serveur RAG opÃ©rationnel"
}
```

### `POST /api/index`
Indexation d'un PDF

**Body** : `FormData` avec fichier PDF (clÃ©: `pdf`)

**RÃ©ponse** :
```json
{
  "ok": true,
  "indexPath": "data/index.json",
  "stats": {
    "chunks": 42,
    "embedModel": "nomic-embed-text",
    "source": "document.pdf"
  }
}
```

### `POST /api/ask`
Question sur le PDF indexÃ©

**Body** :
```json
{
  "question": "Quels sont les objectifs ?"
}
```

**RÃ©ponse** :
```json
{
  "answer": "Les objectifs sont...",
  "sources": [
    {
      "id": "document.pdf::chunk_5",
      "score": 0.89,
      "text": "..."
    }
  ]
}
```

## âš™ï¸ Configuration

### ParamÃ¨tres RAG (dans `rag.js`)

```javascript
// Chunking
chunkSize: 1200        // Taille d'un chunk (caractÃ¨res)
chunkOverlap: 200      // Chevauchement entre chunks

// Retrieval
topK: 5                // Nombre de chunks Ã  rÃ©cupÃ©rer

// ModÃ¨les
embedModel: 'nomic-embed-text'  // ModÃ¨le d'embeddings
chatModel: 'llama3.2'            // ModÃ¨le de gÃ©nÃ©ration
```

### Port du serveur

Par dÃ©faut : `3000`

Pour changer :
```bash
PORT=8080 npm start
```

## ğŸ› DÃ©pannage

### Ollama n'est pas accessible
```bash
# VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# RedÃ©marrer Ollama si nÃ©cessaire
ollama serve
```

### Erreur "pdfParse is not a function"
- Assurez-vous d'avoir la bonne version de `pdf-parse`
- RedÃ©marrez le serveur aprÃ¨s modification

### Le PDF n'est pas indexÃ©
- VÃ©rifiez que le fichier est bien un PDF
- Consultez les logs du serveur pour les erreurs dÃ©taillÃ©es

### RÃ©ponses de mauvaise qualitÃ©
- Augmentez `topK` pour plus de contexte
- Ajustez `chunkSize` et `chunkOverlap`
- Essayez un modÃ¨le LLM plus performant

## ğŸ“š Concepts NLP

### Embeddings
ReprÃ©sentation vectorielle du texte qui capture le sens sÃ©mantique. Des textes similaires auront des vecteurs proches.

### Chunking
DÃ©coupage du texte en morceaux gÃ©rables par le LLM. L'overlap Ã©vite de perdre le contexte aux frontiÃ¨res.

### RAG vs Fine-tuning
- **RAG** : Pas besoin de rÃ©entraÃ®ner, ajout dynamique de connaissances
- **Fine-tuning** : RÃ©entraÃ®nement coÃ»teux, connaissances figÃ©es

### Vector Store
Base de donnÃ©es contenant les chunks avec leurs embeddings pour une recherche rapide.

## ğŸ“ Ressources

- [Ollama Documentation](https://ollama.ai/docs)
- [RAG Explained](https://arxiv.org/abs/2005.11401)
- [Langchain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

## ğŸ‘¤ Auteur

**Imen Boussetta**

Projet dÃ©veloppÃ© dans le cadre d'une formation en Intelligence Artificielle (LLM, LangChain, RAG, Agents).

## ğŸ“ Licence

Ce projet est Ã  usage Ã©ducatif.

---

Fait avec â¤ï¸ en JavaScript et Ollama
